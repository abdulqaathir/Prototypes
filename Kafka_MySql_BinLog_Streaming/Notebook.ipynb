{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0d8ab08-17f0-4385-a8b2-602dc74fd7b5",
   "metadata": {},
   "source": [
    "### **Debezium and Kafka Connect: Overview and How They're Linked**\n",
    "\n",
    "**Kafka Connect** and **Debezium** work together to capture and stream **real-time data changes** from databases (Change Data Capture, CDC) into Kafka. Here’s a breakdown of each and how they link together:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Kafka Connect**\n",
    "- **Kafka Connect** is a tool for **integrating** **Kafka** with external systems such as databases, data lakes, message queues, and more.\n",
    "- **Kafka Connect** makes it easier to stream data between **Kafka** and other systems by using **connectors**.\n",
    "  - It is designed to scale and handle **data integration** in a **distributed, fault-tolerant** manner.\n",
    "  - It abstracts out the low-level work of consuming and producing messages in Kafka, so developers don’t need to focus on handling Kafka’s internal complexities.\n",
    "\n",
    "#### Key Features:\n",
    "- **Pre-built Connectors**: Kafka Connect provides a large set of connectors (such as JDBC, Elasticsearch, etc.) that enable integration with various external systems.\n",
    "- **Source and Sink Connectors**:\n",
    "  - **Source connectors** pull data from an external system into Kafka.\n",
    "  - **Sink connectors** push data from Kafka into an external system.\n",
    "- **Distributed Mode**: Kafka Connect can run in **standalone mode** (for small workloads) or **distributed mode** (for larger, scalable workloads).\n",
    "- **Fault Tolerance**: It provides **fault tolerance** and **offset tracking** for the data ingestion process.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Debezium**\n",
    "- **Debezium** is an open-source project that provides **Change Data Capture (CDC)** capabilities for **relational databases** (and more).\n",
    "- It is built on top of **Kafka Connect**, so it works as a **connector** in Kafka Connect’s ecosystem.\n",
    "- **Debezium** listens for changes in the database (insert, update, delete) and streams these changes in **real time** to **Kafka topics**.\n",
    "  - It uses **database logs** (binlogs in MySQL, WAL in PostgreSQL) to track data changes, making it very efficient.\n",
    "  - It can also capture schema changes (DDL operations like `CREATE`, `ALTER`, `DROP`).\n",
    "  \n",
    "#### Key Features:\n",
    "- **Real-time CDC**: Debezium captures data changes **in real-time**, ensuring that your Kafka topics stay synchronized with the state of the database.\n",
    "- **Database Support**: It supports popular databases such as MySQL, PostgreSQL, SQL Server, MongoDB, etc.\n",
    "- **Schema History**: It tracks schema changes, meaning that if the structure of the data changes (e.g., a column is added), Debezium captures that too.\n",
    "- **Topic Structure**: Each table in the database corresponds to a Kafka topic with the format `<server-name>.<database-name>.<table-name>`.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Kafka Connect and Debezium Are Linked**\n",
    "- **Debezium is a Kafka Connect connector**:\n",
    "  - Kafka Connect provides the **framework** for managing connectors, while Debezium is a **specific connector** that handles the change data capture from databases.\n",
    "  - Debezium works **inside Kafka Connect** to consume database changes and push them to Kafka topics.\n",
    "- **Kafka Connect is the orchestration layer**: It manages the lifecycle, scalability, and fault tolerance of the Debezium connector.\n",
    "- **Debezium uses Kafka Connect’s infrastructure**:\n",
    "  - Debezium leverages Kafka Connect’s features like offset management, fault tolerance, and distributed execution.\n",
    "  - It also uses the Kafka Connect **REST API** to configure and manage the connector instances.\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use Kafka Connect and Debezium**\n",
    "\n",
    "#### **1. Use Kafka Connect When:**\n",
    "- You need to **integrate Kafka with other systems** (e.g., databases, message queues, Elasticsearch).\n",
    "- You require **scalability** for data integration.\n",
    "- You need **fault tolerance** in your data pipelines.\n",
    "- You want a **centralized framework** for managing connectors without having to handle low-level Kafka operations.\n",
    "- You need **simple data extraction and loading** from systems with existing Kafka connectors.\n",
    "\n",
    "#### **2. Use Debezium When:**\n",
    "- You want to **capture real-time changes** (CDC) from a database.\n",
    "- You need to **stream database changes to Kafka** with minimal latency (e.g., **insertions, updates, and deletions**).\n",
    "- You want to **preserve schema changes** (like `CREATE`, `ALTER`, `DROP`).\n",
    "- You need to **stream data into downstream systems** (data lakes, search engines, etc.) using Kafka topics.\n",
    "- You want **fine-grained control over which tables or databases** to monitor for changes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Use Case**\n",
    "Let’s consider a scenario where you have an e-commerce platform with a MySQL database that stores customer data. You want to:\n",
    "\n",
    "1. **Stream all customer updates** to a Kafka topic in real-time.\n",
    "2. **Maintain a record** of all schema changes (e.g., adding new fields in the `customers` table).\n",
    "3. **Push data from Kafka** to a downstream analytics system.\n",
    "\n",
    "In this case:\n",
    "- **Kafka Connect** would manage the **Debezium connector**.\n",
    "- **Debezium** would be the connector that captures **real-time changes** in the `customers` table and streams them to Kafka.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- **Kafka Connect**: Manages connectors to integrate Kafka with external systems. It provides the framework for running and scaling connectors.\n",
    "- **Debezium**: A specific Kafka Connect connector that captures **real-time database changes** (CDC) from databases like MySQL, PostgreSQL, and others.\n",
    "\n",
    "**When to Use Kafka Connect**: When you need to connect Kafka to external systems (databases, message queues, etc.) in a scalable and fault-tolerant manner.\n",
    "\n",
    "**When to Use Debezium**: When you need to capture **real-time changes** from a database and stream them to Kafka topics for further processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aa73bc3-fa24-4a33-80b4-4232e63243ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 201\n",
      "Response: {'name': 'mysql-connector', 'config': {'connector.class': 'io.debezium.connector.mysql.MySqlConnector', 'tasks.max': '1', 'database.hostname': 'mysql', 'database.port': '3306', 'database.user': 'root', 'database.password': 'root', 'database.server.id': '1', 'database.server.name': '5d09a0c732cb', 'database.include.list': 'testdb', 'topic.prefix': 'dbserver1', 'schema.history.internal.kafka.topic': 'dbz-internal', 'schema.history.internal.kafka.bootstrap.servers': 'kafka:9092', 'name': 'mysql-connector'}, 'tasks': [], 'type': 'source'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:8083/connectors\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "data = {\n",
    "    \"name\": \"mysql-connector\",\n",
    "    \"config\": {\n",
    "        \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n",
    "        \"tasks.max\": \"1\",\n",
    "        \"database.hostname\": \"mysql\",\n",
    "        \"database.port\": \"3306\",\n",
    "        \"database.user\": \"root\",\n",
    "        \"database.password\": \"root\",\n",
    "        \n",
    "        \"database.server.id\": \"1\",\n",
    "        \"database.server.name\": \"5d09a0c732cb\",\n",
    "        \n",
    "        \"database.include.list\": \"testdb\",      \n",
    "        \"topic.prefix\": \"dbserver1\",  # Add the topic.prefix here,\n",
    "        \n",
    "        \"schema.history.internal.kafka.topic\": \"dbz-internal\", # Stores schema changes basically all ddls\n",
    "        \"schema.history.internal.kafka.bootstrap.servers\": \"kafka:9092\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "# Print response status and data\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Response: {response.json()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd6e28-8d9f-4408-a46a-45577a9dbb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ced4fe-087d-4223-a5f0-6204e13d9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use testdb;\n",
    "CREATE TABLE users1 (\n",
    "  id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "  name VARCHAR(255),\n",
    "  email VARCHAR(255)\n",
    ");\n",
    "\n",
    "INSERT INTO users1 (name, email) VALUES ('John Doe', 'john.doe@example.com');\n",
    "INSERT INTO users1 (name, email) VALUES ('Jane Doe', 'jane.doe@example.com');\n",
    "Select * from testdb.users1;\n",
    "\n",
    "Truncate table testdb.users1; // Doesnt log anything in binlog\n",
    "\n",
    "Delete from testdb.users1 where id<10;\n",
    "\n",
    "Update testdb.users1\n",
    "Set name='MAQ' where id >=5;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3467499e-3566-493d-8f45-61eb80f773b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started\n",
      "{'before': None, 'after': {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384803000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 651, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384803717, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 2, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384803000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 975, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384803719, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 3, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384809000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 1299, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384809710, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 4, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384809000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 1623, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384809717, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 5, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384810000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 1947, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384810081, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 6, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384810000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 2271, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384810087, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 7, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384810000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 2595, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384810317, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': None, 'after': {'id': 8, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384810000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 2919, 'row': 0, 'thread': 9, 'query': None}, 'op': 'c', 'ts_ms': 1741384810323, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': {'id': 5, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'after': {'id': 5, 'name': 'MAQ', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384814000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 3252, 'row': 0, 'thread': 9, 'query': None}, 'op': 'u', 'ts_ms': 1741384814022, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': {'id': 6, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'after': {'id': 6, 'name': 'MAQ', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384814000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 3252, 'row': 1, 'thread': 9, 'query': None}, 'op': 'u', 'ts_ms': 1741384814022, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': {'id': 7, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'after': {'id': 7, 'name': 'MAQ', 'email': 'john.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384814000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 3252, 'row': 2, 'thread': 9, 'query': None}, 'op': 'u', 'ts_ms': 1741384814022, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': {'id': 8, 'name': 'Jane Doe', 'email': 'jane.doe@example.com'}, 'after': {'id': 8, 'name': 'MAQ', 'email': 'jane.doe@example.com'}, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384814000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 3252, 'row': 3, 'thread': 9, 'query': None}, 'op': 'u', 'ts_ms': 1741384814022, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "{'before': {'id': 1, 'name': 'John Doe', 'email': 'john.doe@example.com'}, 'after': None, 'source': {'version': '2.5.4.Final', 'connector': 'mysql', 'name': 'dbserver1', 'ts_ms': 1741384821000, 'snapshot': 'false', 'db': 'testdb', 'sequence': None, 'table': 'users1', 'server_id': 1, 'gtid': None, 'file': 'binlog.000002', 'pos': 3816, 'row': 0, 'thread': 9, 'query': None}, 'op': 'd', 'ts_ms': 1741384821445, 'transaction': None}\n",
      "\n",
      "\n",
      "\n",
      "Error\n",
      "<cimpl.Message object at 0x000001BBF1B80A40>\n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "Error\n",
      "<cimpl.Message object at 0x000001BBF1B819C0>\n",
      "b'Subscribed topic not available: dbserver1.testdb.users1: Broker: Unknown topic or partition'\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(msg\u001b[38;5;241m.\u001b[39mvalue())\n\u001b[0;32m     45\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m msg\u001b[38;5;241m.\u001b[39mvalue() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 46\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     47\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    341\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 345\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    361\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "import json\n",
    "\n",
    "# Configure the Kafka consumer\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9094',\n",
    "    'group.id': 'test1-group',\n",
    "    'auto.offset.reset': 'earliest',\n",
    "    'security.protocol': 'PLAINTEXT',\n",
    "    'enable.auto.commit': False\n",
    "}\n",
    "\n",
    "# Create a Kafka consumer\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topic\n",
    "consumer.subscribe(['dbserver1.testdb.users1'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Started\")\n",
    "# Consume messages\n",
    "try:\n",
    "    while True:\n",
    "        try:\n",
    "            msg = consumer.poll(timeout=1.0)\n",
    "            \n",
    "            if msg is None:\n",
    "                continue\n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise KafkaException(msg.error())\n",
    "            \n",
    "            # Parse the message value (which is in JSON format)\n",
    "            message = json.loads(msg.value().decode('utf-8'))\n",
    "            print(message[\"payload\"])\n",
    "            print(\"\\n\\n\")\n",
    "        except Exception as e:\n",
    "            print(\"Error\")\n",
    "            print(msg)\n",
    "            if msg is not None:\n",
    "                print(msg.value())\n",
    "            if msg.value() is not None:\n",
    "                print(json.loads(msg.value().decode('utf-8')))\n",
    "            print(\"\\n\\n\")\n",
    "finally:\n",
    "    consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "493d38cb-3922-40ee-bb3a-fae8b10a2583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a6bdaf1-e76f-4814-b042-befc5d222fd3",
   "metadata": {},
   "source": [
    "Yes, your understanding is correct! The way **binlog** (binary log) works in MySQL and other databases like PostgreSQL with **WAL (Write-Ahead Logs)** is one of the reasons why **batching deletes** is highly recommended, especially when dealing with a large number of rows.\n",
    "\n",
    "### **How Binlog Works for DML Changes (Including Deletes)**\n",
    "The binlog captures **every Data Manipulation Language (DML)** operation—`INSERT`, `UPDATE`, `DELETE`—and writes the **before** and **after** states of each modified row, including:\n",
    "\n",
    "- **For `INSERT`**: The new row is logged.\n",
    "- **For `UPDATE`**: The row before the update (old data) and the row after the update (new data) are logged.\n",
    "- **For `DELETE`**: The row that is deleted is logged (essentially the \"before\" state).\n",
    "\n",
    "Each of these events generates a log entry for **each modified row**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Deleting Millions of Rows in One Shot is Problematic**\n",
    "\n",
    "1. **Binlog Size and Kafka Topic Load:**\n",
    "   - If you delete **millions of rows in one shot**, each row deletion creates a binlog entry.\n",
    "   - This can **explode the size of your binlog** and result in an overwhelming amount of data being written to Kafka.\n",
    "   - Kafka topics may become **overloaded** with a massive number of deletion events.\n",
    "   - **Kafka consumers** (e.g., the downstream systems or other connectors) will also face high load when processing these large numbers of events.\n",
    "\n",
    "2. **Performance Impact:**\n",
    "   - **Processing millions of binlog entries** in a short period can have **performance impacts** on the database and Kafka Connect.\n",
    "   - Kafka Connect connectors, especially **Debezium**, can get overwhelmed, leading to delays in data processing or potential **timeouts**.\n",
    "   - The binlog itself might grow too large, **hitting storage limits** or causing issues with the **retention period** for logs.\n",
    "\n",
    "3. **Real-Time Data Processing Concerns:**\n",
    "   - If you're using **real-time data processing**, like stream processing systems (e.g., **Kafka Streams**, **Apache Flink**, etc.), **millions of deletes in one go** could create huge backlogs, affecting your processing pipeline’s **latency** and **throughput**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Batching Deletes Helps**\n",
    "\n",
    "**Batching deletes** (i.e., deleting rows in smaller groups, say 1000 or 10,000 rows at a time) helps reduce the **volume of binlog entries** and the stress on Kafka Connect and downstream consumers.\n",
    "\n",
    "1. **Smaller Binlog Entries**:\n",
    "   - Instead of writing millions of deletion entries to the binlog, you are writing smaller, more manageable batches.\n",
    "   - This helps keep the **binlog size manageable** and reduces the strain on Kafka topics.\n",
    "   \n",
    "2. **Reduced Load on Kafka**:\n",
    "   - Kafka Connect and downstream consumers only need to process a smaller batch of messages at a time, which prevents large spikes in memory or CPU usage.\n",
    "   - **Faster processing** as smaller batches mean quicker processing times.\n",
    "\n",
    "3. **Improved Fault Tolerance**:\n",
    "   - When working with large datasets, **batching reduces the chances of failures** due to timeouts or overloads in Kafka Connect or other systems.\n",
    "   - If an issue occurs while processing a batch, it's easier to recover from and reprocess a smaller set of events.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Example:**\n",
    "Let's consider you have a **table with 1 million rows** to delete.\n",
    "\n",
    "- **Without Batching**:  \n",
    "  - You issue a single `DELETE FROM my_table WHERE condition`.  \n",
    "  - This generates **1 million entries** in the binlog (1 for each row).\n",
    "  - Kafka Connect will capture these and publish them to Kafka, overwhelming Kafka consumers and causing high processing latencies.\n",
    "\n",
    "- **With Batching**:  \n",
    "  - You break the delete operation into smaller batches like:\n",
    "    ```sql\n",
    "    DELETE FROM my_table WHERE condition LIMIT 10000;\n",
    "    ```\n",
    "  - This results in **100 batches** of 10,000 rows each.\n",
    "  - Each batch generates **fewer binlog entries**, making it more manageable for Kafka Connect and Kafka consumers.\n",
    "  - Kafka consumers process **one batch at a time**, reducing the load and improving the overall system performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Recommendations:**\n",
    "\n",
    "- **Limit Deletes to Smaller Batches**: If you have to delete a large number of rows, **batch the deletes** into manageable chunks (e.g., 10,000 rows per batch) rather than executing a single large `DELETE`.\n",
    "  \n",
    "- **Monitor Binlog and Kafka Performance**: Always monitor the size of your binlog and the load on Kafka to ensure the system remains scalable and responsive.\n",
    "\n",
    "- **Consider Soft Deletes**: For very large datasets, consider using **soft deletes** (where rows are marked as deleted but not physically removed) and then handle purging in the background later. This approach avoids generating a large volume of DML logs.\n",
    "\n",
    "- **Test Before Full Deployment**: Always test with smaller datasets to gauge the load and performance impact before applying this to larger production systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "Batching deletes helps alleviate the pressure on the binlog, Kafka, and downstream systems by reducing the number of log entries generated. Since binlogs record both the before and after state of each row (for each DML operation), large-scale deletes can significantly increase system load, and therefore it's advised to batch them for more efficient handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb0c300-f82d-489e-8e01-b43e96142056",
   "metadata": {},
   "source": [
    "### Write-Ahead Log (WAL) and Binary Log (Binlog): A Detailed Comparison\n",
    "\n",
    "#### **Write-Ahead Log (WAL)**\n",
    "\n",
    "A **Write-Ahead Log (WAL)** is a logging mechanism used primarily by databases to ensure data integrity. It is a technique where all modifications to the database are written to a log file before the changes are applied to the actual database files. This ensures that in the event of a crash or power failure, the database can recover to a consistent state by replaying the log.\n",
    "\n",
    "**Key Characteristics of WAL**:\n",
    "- **Atomicity and Durability (ACID)**: WAL guarantees atomicity and durability. When a transaction is committed, the corresponding log entry is first written to the WAL and then applied to the database. This ensures that even if a failure occurs after the log is written but before the data is fully updated, the transaction can be replayed to achieve consistency.\n",
    "- **Crash Recovery**: In case of a crash, the database can replay the WAL to ensure all committed transactions are applied. This is especially useful for ensuring that no committed transaction is lost.\n",
    "- **Incremental Updates**: WAL stores each change to the database as a log entry, typically in a form of a \"log record\" that describes what data was changed and how.\n",
    "- **Performance**: While WAL ensures durability, it can impact performance because every modification must first be logged, potentially slowing down write-heavy operations. However, WAL helps optimize read operations since it allows for efficient recovery and ensures that data is written in a controlled and sequential manner.\n",
    "\n",
    "**WAL and Consistent System State**:\n",
    "WAL is critical for maintaining consistency in a system. Typically, the process works as follows:\n",
    "\n",
    "1. **Checkpointing**: Periodically, the system will create a **checkpoint**, which is a snapshot of the persisted state of the database.\n",
    "2. **Non-persistent Operations**: These are operations or transactions that have not yet been written to the actual data files. These operations are still in memory or in a buffer.\n",
    "3. **WAL Entries**: The WAL records all changes to the database, including both persistent operations (that affect disk data directly) and non-persistent ones (such as in-memory updates).\n",
    "4. **Crash Recovery**: If a crash occurs, the system can replay the WAL from the last checkpoint. It will apply the changes in the WAL that were not yet persisted, effectively \"replaying\" in-memory operations and ensuring the system state is consistent with the last known checkpoint.\n",
    "\n",
    "This ensures that the system reaches a consistent state by reapplying the operations in the WAL up to the point of failure, so no transactions are lost.\n",
    "\n",
    "**Does WAL store one entry per row affected?**\n",
    "WAL typically does not store one entry per row affected but rather stores changes at a higher level of abstraction, often at the page or block level. In most databases, the WAL records are designed to capture **logical operations** such as:\n",
    "- Inserting a new row\n",
    "- Updating a column\n",
    "- Deleting a row\n",
    "\n",
    "These log entries describe the operation and may include sufficient information to apply it during recovery. However, the granularity can vary based on the system implementation. Some systems may record more detailed information, such as the specific row or column affected, but this is not always the case. For example, in a relational database, the WAL may log an insert statement with the data being inserted, not individual row-level operations.\n",
    "\n",
    "#### **Binary Log (Binlog)**\n",
    "\n",
    "The **Binary Log (Binlog)** is a log format used by MySQL and similar databases. It is primarily used for replication purposes, where changes made to the primary database are logged and then propagated to replica databases. Unlike WAL, the Binlog is not strictly about durability but focuses on replication and recovery.\n",
    "\n",
    "**Key Characteristics of Binlog**:\n",
    "- **Replication**: The primary use case for the Binlog is replication. It logs all changes made to the database (inserts, updates, deletes) in a binary format that can be easily sent to replica databases.\n",
    "- **Transaction Support**: Binlogs capture the start and end of transactions, and they can be replayed on replica servers to keep them synchronized with the master.\n",
    "- **Event-Based**: The Binlog records database events in a compact binary format. These events can include both data changes and schema changes.\n",
    "- **Point-in-Time Recovery**: Binlogs can be used for point-in-time recovery by replaying specific parts of the log. This allows a database to recover to a specific point by applying all events up to that moment.\n",
    "\n",
    "**Differences Between WAL and Binlog**:\n",
    "1. **Purpose**:\n",
    "   - WAL is primarily for ensuring durability and crash recovery in databases.\n",
    "   - Binlog is used for replication and point-in-time recovery.\n",
    "   \n",
    "2. **Format**:\n",
    "   - WAL logs tend to be written in a format that is optimized for recovery (often in a sequential, readable format).\n",
    "   - Binlog uses a binary format optimized for replication and storage efficiency.\n",
    "\n",
    "3. **Consistency**:\n",
    "   - WAL focuses on maintaining a consistent state of the database after a crash or failure.\n",
    "   - Binlog focuses on replicating changes to other servers, ensuring consistency across the cluster.\n",
    "\n",
    "4. **Granularity**:\n",
    "   - WAL typically logs at the page/block level, although some databases might log at a finer granularity.\n",
    "   - Binlog logs individual SQL statements or database events, making it more granular and suited for replication.\n",
    "\n",
    "5. **Replication**:\n",
    "   - WAL does not serve the purpose of replication, though it plays a critical role in recovery and ensuring data consistency.\n",
    "   - Binlog, on the other hand, is integral to MySQL replication, keeping slaves up-to-date with the master's changes.\n",
    "\n",
    "#### **How WAL is Used to Build Consistent System States**\n",
    "WAL plays a crucial role in building and maintaining the consistent state of a system. Here's how the process works:\n",
    "\n",
    "1. **Transaction Begin**: A new transaction starts, and any changes to the database are first written to the WAL.\n",
    "2. **Non-Persistent Changes**: The changes are made in-memory or on pages that are not yet written to disk.\n",
    "3. **Checkpoint**: A checkpoint is created at some point, where the system writes a consistent snapshot of the database state to disk.\n",
    "4. **WAL Replay**: After a system crash or failure, the WAL is replayed from the last checkpoint. The system applies all the changes in the WAL that were made after the checkpoint, including any non-persistent changes.\n",
    "5. **Consistency Restored**: By replaying the WAL, the system reaches a consistent state with all committed transactions applied. This ensures that no data is lost and that the database is in a consistent state even after a failure.\n",
    "\n",
    "In this context, WAL does not store one entry per row affected but rather records logical operations or changes to the database. During recovery, these logical operations are replayed to restore the system to a consistent state.\n",
    "\n",
    "### Conclusion\n",
    "- **WAL** is focused on **durability, crash recovery**, and ensuring **consistent state** in databases, typically logging operations at a higher abstraction level, such as page/block changes.\n",
    "- **Binlog** is focused on **replication** and **point-in-time recovery**, logging changes in a binary format optimized for replication.\n",
    "  \n",
    "Both mechanisms ensure that a system can recover to a consistent state, but WAL is more aligned with crash recovery and data integrity, while Binlog is focused on replication and synchronizing data across multiple servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146dc321-3fcb-4aef-a571-ed7239aa994a",
   "metadata": {},
   "source": [
    "Yes, that's correct. When you **delete** a table (not truncate) with millions of rows, the **delete statement** itself (or a set of delete statements) is logged **logically** in the Write-Ahead Log (WAL) or the Binary Log (Binlog), not the actual deletion of each individual row.\n",
    "\n",
    "Here's how it works in more detail:\n",
    "\n",
    "### Write-Ahead Log (WAL) Behavior:\n",
    "- **Logical Logging**: In WAL, the **delete operation** (whether it’s a single row or a bulk delete) is logged as a **logical operation** that indicates the deletion of data. It will typically log the **SQL statement** (e.g., `DELETE FROM table WHERE condition`) or the change that the database makes when executing that statement.\n",
    "- **Efficient Logging**: Even if the table has millions of rows, **WAL does not log each row individually**. Instead, it logs the change in terms of the **logical operation**: in this case, a `DELETE` command on the specified rows based on the condition. This is efficient and ensures that recovery operations are optimized.\n",
    "- **Recovery**: When a crash occurs and recovery is needed, the system will replay the log to restore the database's state. If the delete was part of a transaction, it will apply the deletion (or undo it) during the recovery process.\n",
    "\n",
    "### Binary Log (Binlog) Behavior (For MySQL):\n",
    "- **Event Logging**: In the case of MySQL and similar databases using the Binlog, a **`DELETE` event** is logged. This event represents the logical operation of deleting rows, not the physical details of each row.\n",
    "- **Granularity**: Depending on the storage engine (e.g., InnoDB), the Binlog may log the `DELETE` operation at a statement level or row level (if using **row-based replication**). However, even in the row-based format, the Binlog logs the deletion **as a logical event** rather than logging each individual row's deletion. It records a reference to the row that was deleted, and if necessary, it can be replicated to another system.\n",
    "\n",
    "### Why This Works:\n",
    "- **Efficiency**: Logging each individual row deletion in a large table (millions of rows) would be highly inefficient and would dramatically increase the size of the log. Instead, logging the **logical operation** (e.g., the `DELETE` statement) allows the system to handle even large deletions efficiently.\n",
    "- **Recovery**: When the system recovers from a crash, it doesn't need to know about each individual row’s deletion; it just needs to apply the **logical operations** that were part of the transaction to get the database back to a consistent state.\n",
    "\n",
    "### Conclusion:\n",
    "When you delete a table with millions of rows, only the **delete statement** (or the logical operation that performs the delete) is logged, not each individual row affected by the delete. This ensures efficient logging and recovery, especially for large datasets, while still guaranteeing the **ACID properties** (Atomicity, Consistency, Isolation, and Durability) of the transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b536b882-3a27-45b0-ae79-4f7d19c01d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.1 Studies Misc",
   "language": "python",
   "name": "studies_misc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
